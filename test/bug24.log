➜  test git:(main) ✗ mpirun --allow-run-as-root -np 1 python -u ./test.ascend.bistabcg-dev56.py
self.lat_size: [16, 16, 16, 16]
self.grid_size: [1, 1, 1, 1]
self.grid_index: [0, 0, 0, 0]
self.local_lat_size: [16, 16, 16, 16]
Using device: cpu
@My Rank:0/1, Local Rank:0@

self.wilson.check_su3(self.U): True
block.shape: (3, 3, 4, 16, 16, 16, 16)
grid_size: [1, 1, 1, 1]
grid_index_t, grid_index_z, grid_index_y, grid_index_x: (0, 0, 0, 0)
block.shape: (4, 3, 4, 3, 16, 16, 16, 16)
grid_size: [1, 1, 1, 1]
grid_index_t, grid_index_z, grid_index_y, grid_index_x: (0, 0, 0, 0)
block.shape: (4, 3, 16, 16, 16, 16)
grid_size: [1, 1, 1, 1]
grid_index_t, grid_index_z, grid_index_y, grid_index_x: (0, 0, 0, 0)
block.shape: (4, 3, 16, 16, 16, 16)
grid_size: [1, 1, 1, 1]
grid_index_t, grid_index_z, grid_index_y, grid_index_x: (0, 0, 0, 0)
self.dof_list:[12, 24, 24, 24, 24]
Building grid list:
  Level 0: 16x16x16x16
  Level 1: 8x8x8x8
  Level 2: 4x4x4x4
  Level 3: 2x2x2x2
self.grid_list:[[16, 16, 16, 16], [8, 8, 8, 8], [4, 4, 4, 4], [2, 2, 2, 2]]
self.dof_list:[12, 24, 24, 24, 24]
Building grid list:
  Level 0: 16x16x16x16
  Level 1: 8x8x8x8
  Level 2: 4x4x4x4
  Level 3: 2x2x2x2
self.grid_list:[[16, 16, 16, 16], [8, 8, 8, 8], [4, 4, 4, 4], [2, 2, 2, 2]]
Input Tensor Shape: torch.Size([4, 3, 16, 16, 16, 16])
Grid Index T: 0, Z: 0, Y: 0, X: 0
Grid Lat T: 16, Z: 16, Y: 16, X: 16
Dest Shape: (4, 3, 16, 16, 16, 16)
rank 0: Data is saved to test.ascend-dev56b.h5
Input Tensor Shape: torch.Size([3, 3, 4, 16, 16, 16, 16])
Grid Index T: 0, Z: 0, Y: 0, X: 0
Grid Lat T: 16, Z: 16, Y: 16, X: 16
Dest Shape: (3, 3, 4, 16, 16, 16, 16)
rank 0: Data is saved to test.ascend-dev56U.h5
Input Tensor Shape: torch.Size([4, 3, 4, 3, 16, 16, 16, 16])
Grid Index T: 0, Z: 0, Y: 0, X: 0
Grid Lat T: 16, Z: 16, Y: 16, X: 16
Dest Shape: (4, 3, 4, 3, 16, 16, 16, 16)
rank 0: Data is saved to test.ascend-dev56clover_term.h5
src_plus.shape,hopping.shape: (torch.Size([12, 16, 16, 16, 16]), torch.Size([12, 12, 8, 8, 8, 8]))
Traceback (most recent call last):
  File "/root/PyQCU/test/./test.ascend.bistabcg-dev56.py", line 7, in <module>
    _qcu.solve()
  File "/root/PyQCU/test/pyqcu/ascend/__init__.py", line 147, in solve
    x = bicgstab(b=self.b.clone() if b == None else b.clone(), matvec=self.matvec, tol=self.tol, max_iter=self.max_iter,
  File "/root/PyQCU/test/pyqcu/ascend/inverse.py", line 109, in bicgstab
    r = b - matvec(x)
  File "/root/PyQCU/test/pyqcu/ascend/__init__.py", line 121, in matvec
    return self.op.matvec(src).clone()
  File "/root/PyQCU/test/pyqcu/ascend/inverse.py", line 479, in matvec
    return (self.hopping.matvec(src=src.reshape([12]+list(src.shape)[2:]))+self.sitting.matvec(src=src.reshape([12]+list(src.shape)[2:]))).reshape([4, 3]+list(src.shape)[2:])
  File "/root/PyQCU/test/pyqcu/ascend/inverse.py", line 380, in matvec
    dest += self.matvec_plus(ward=ward, src=src)
  File "/root/PyQCU/test/pyqcu/ascend/inverse.py", line 360, in matvec_plus
    return self.wilson.give_wilson_plus(ward=ward, src=src, hopping=self.M_plus_list[ward], src_tail=src_tail)
  File "/root/PyQCU/test/pyqcu/ascend/dslash.py", line 579, in give_wilson_plus
    return torch.einsum(
  File "/usr/local/lib/python3.10/dist-packages/torch/functional.py", line 402, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
RuntimeError: einsum(): subscript t has size 16 for operand 1 which does not broadcast with previously seen size 8
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[1781,1],0]
  Exit code:    1
--------------------------------------------------------------------------
